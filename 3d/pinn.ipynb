{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f254ad35-9a6e-470b-b385-8dc6f5096b0a",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2021 alxyok\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7319751-bf8a-48a6-95b2-2120b324a124",
   "metadata": {},
   "source": [
    "# Solving the 3D Burger's equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe5a703-9683-4511-9914-73ec73464161",
   "metadata": {},
   "source": [
    "**FORGET ABOUT THIS CODE, IT'S UNTRACTABLE. FOR EDUCATIONAL PURPOSES ONLY.**\n",
    "\n",
    "**Let's have some fun! Untested fun even ðŸ˜› (It works, but what it really does no one knows for certain!)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ce73c8-e31d-433d-b7ce-7c990fc96430",
   "metadata": {},
   "source": [
    "**Most of the content below is largely inspired from the work of [Raissi et al.](https://maziarraissi.github.io/PINNs/) as weel as [Liu et al.](https://www.sciencedirect.com/science/article/abs/pii/S0142727X21000527), please refer to those papers for a comprehensive theoretical understanding.**\n",
    "\n",
    "The Burger's equation is one of the well-studied fundamental PDEs in that exhibit shocks, and for which an non-trivial analytical solution exists in the Physics litterature. A conjunction of factors (profusion of data, capable cheap hardware, and backprop) has lead to the resurection Deep Learning (DL) which has in turn paved the way for the development of scientific machine learning libraries such as TensorFlow and PyTorch. \n",
    "\n",
    "Those frameworks come with free auto-differentiation, a key tool for this lab which will enable the development of a self-supervised neural model based on residuals.\n",
    "\n",
    "We'll use PyTorch, but TensorFlow + Keras could do just as fine. Be sure to check out [PyTorch Tutorials](https://pytorch.org/tutorials/) and [PyTorch API](https://pytorch.org/docs/1.9.0/), which are a great source of information. Also, [Stackoverflow](https://stackoverflow.com/questions/tagged/pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e861d578-c4a7-4161-8a1f-e456dfdb63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7fb697d8-0536-4b32-9e04-3d0814ee15c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# we'll use PyTorch, but TensorFlow + Keras could do just as fine\n",
    "import torch\n",
    "\n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4340e5d6-8703-4922-9e48-f174cbbaef08",
   "metadata": {},
   "source": [
    "## 1. Problem statement\n",
    "\n",
    "Note: we do not use the Hopf-Cole transformation that would allow for a simplified formula but instead use the raw explicit formulation of the problem. \n",
    "\n",
    "We propose to solve the 3D nonlinear Burger's problem defined by the following set of equations:\n",
    "\n",
    "$u_t + u * u_x + v * u_y + w * u_z - \\frac{1}{R_e} (u_{xx} + u_{yy} + u_{zz}) = 0$\n",
    "\n",
    "$v_t + u * v_x + v * v_y + w * v_z - \\frac{1}{R_e} (v_{xx} + v_{yy} + v_{zz}) = 0$\n",
    "\n",
    "$w_t + u * w_x + v * w_y + w * w_z - \\frac{1}{R_e} (w_{xx} + w_{yy} + w_{zz}) = 0$\n",
    "\n",
    "in which $Re$ is the Reynolds number, which characterizes the fluid flow behavior in various situations, and under the initial condition and boundary conditions defined below. The space domain is $0 < x, y, z < 1$ and time domain is $t > 0$.\n",
    "\n",
    "$u(x, y, z, 0) = u_0(x, y, z) = sin(\\pi x) * cos(\\pi y) * cos(\\pi z)$\n",
    "\n",
    "$v(x, y, z, 0) = v_0(x, y, z) = sin(\\pi y) * cos(\\pi x) * cos(\\pi z)$\n",
    "\n",
    "$w(x, y, z, 0) = w_0(x, y, z) = sin(\\pi z) * cos(\\pi x) * cos(\\pi y)$\n",
    "\n",
    "as well as:\n",
    "\n",
    "$u(0, y, z, t) = u(1, y, z, t) = 0$\n",
    "\n",
    "$v(x, 0, z, t) = v(x, 1, z, t) = 0$\n",
    "\n",
    "$w(x, y, 0, t) = w(x, y, 1, t) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fee474-97c1-42ca-9abb-82ce4f0ffd2b",
   "metadata": {},
   "source": [
    "## 2. The resolution method\n",
    "\n",
    "We will build an estimator and have it gradually converge to the 3-tuple solution $U = (u, v, w)$ thanks to a handcrafted loss function based on residuals, computed from original inputs $X = (x, y, z, t)$.\n",
    "\n",
    "We define:\n",
    "\n",
    "* A neural model $pinn := U(x, y, z, t)$\n",
    "* An IC residual function $U0_{residual} := pinn(X, 0) - U0(X)$\n",
    "* A BC residuals function $Ulim_{residual} := U(0, t) = U(1, t) = 0$\n",
    "* A PDE residual function $f := U_t + U * U_{.} - \\frac{1}{R_e} * U_{..}$\n",
    "\n",
    "The Physics constraint is a soft-constraint (based on the loss) built by summing the loss of all residuals $L = loss(U0) + loss(Ulim) + loss(f)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc195742-bc8c-410b-b2d4-dae03adf6cb5",
   "metadata": {},
   "source": [
    "#### A few of the model's HParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac49f540-a2b1-4bd8-abe9-2ba8a4b42c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples in every dimension\n",
    "n = 4\n",
    "grid_shape = (n, n, n, n)\n",
    "\n",
    "dtype = torch.float\n",
    "\n",
    "# reynolds number, try for a range of 10^p where p is an integer \n",
    "re: float = 100.\n",
    "\n",
    "# learning rate, classic\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f16361-966b-49ac-b6d8-5047c6278683",
   "metadata": {},
   "source": [
    "#### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45c5fa9f-d77a-4889-bdc2-b3846f2472d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuplify(X: torch.Tensor) -> tuple:\n",
    "    \n",
    "    x = X[:, 0:1]\n",
    "    y = X[:, 1:2]\n",
    "    z = X[:, 2:3]\n",
    "    t = X[:, 3:4]\n",
    "    \n",
    "    return x, y, z, t\n",
    "\n",
    "def meshify(X: torch.Tensor) -> torch.Tensor:\n",
    "    x, y, z, t = tuplify(X)\n",
    "    x, y, z, t = np.meshgrid(x, y, z, t)\n",
    "    \n",
    "    x = torch.tensor(x.reshape((-1, 1)))\n",
    "    y = torch.tensor(y.reshape((-1, 1)))\n",
    "    z = torch.tensor(z.reshape((-1, 1)))\n",
    "    t = torch.tensor(t.reshape((-1, 1)))\n",
    "    \n",
    "    X = torch.squeeze(torch.stack((x, y, z, t), axis=1))\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5276047-2d94-4e7f-bc82-ce26d6df739f",
   "metadata": {},
   "source": [
    "## 3. The actual implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc0e59-6626-4a11-a4c1-16a85dcd4e94",
   "metadata": {},
   "source": [
    "#### a) IC residuals function\n",
    "\n",
    "Following the article specifications, we'll define the IC with a few cyclical functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a230f085-02c4-4155-8e58-0351eb22cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def U0(X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Computes the IC as stated previously.\"\"\"\n",
    "    \n",
    "    # X = meshify(X)\n",
    "    x, y, z, _ = tuplify(X)\n",
    "    \n",
    "    u_xyz0 = torch.squeeze(torch.sin(np.pi * x) * torch.cos(np.pi * y) * torch.cos(np.pi * z))\n",
    "    v_xyz0 = torch.squeeze(torch.sin(np.pi * y) * torch.cos(np.pi * x) * torch.cos(np.pi * z))\n",
    "    w_xyz0 = torch.squeeze(torch.sin(np.pi * z) * torch.cos(np.pi * x) * torch.cos(np.pi * y))\n",
    "    \n",
    "    U0_ = torch.stack((u_xyz0, v_xyz0, w_xyz0), axis=1)\n",
    "    \n",
    "    return U0_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac8a6f-7296-4a9a-bdf1-03e692eebbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def U0_residuals(X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Computes the residuals for the IC.\"\"\"\n",
    "    \n",
    "    return pinn(X) - U0(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d346205-fae9-49fd-bfde-7552a96cb53d",
   "metadata": {},
   "source": [
    "#### b) BC residuals function\n",
    "\n",
    "Residuals on boundary is `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00f0ce6c-b215-4bc8-9ce9-ab0a2f07ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ulim_residuals(X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Computes the residuals at the Boundary.\"\"\"\n",
    "    \n",
    "    return pinn(X) - 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35a1ed-9d06-406a-8da9-631491b921d5",
   "metadata": {},
   "source": [
    "#### c) PDE residuals function\n",
    "\n",
    "We need to compute first-order and second-order derivatives of $U$ with respect to $X$. Currently, `torch.__version__ == 1.9.0`, it's a bit tricky, because we cannot filter out *a priori* part of terms that will end-up unused and thus computation is partly wasted. We can only filter *a posteriori*. There's probably some leverage at the DAG level *(Directed Acyclic Graph)*.\n",
    "\n",
    "PyTorch has a `torch.autograd.functional.hessian()` function but only of output scalars and not vectors so we can't use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "910f10b0-1715-4e8e-a10c-43de9bea0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Computes the residuals from the PDE on the rest of the Domain.\"\"\"\n",
    "    \n",
    "    def first_order(X, second_order=False):\n",
    "        \n",
    "        U = pinn(X)\n",
    "        u = torch.squeeze(U[:, 0:1])\n",
    "        v = torch.squeeze(U[:, 1:2])\n",
    "        w = torch.squeeze(U[:, 2:3])\n",
    "        \n",
    "        U_X = torch.autograd.functional.jacobian(pinn, X, create_graph=True)\n",
    "        \n",
    "        u_x = torch.diagonal(torch.squeeze(U_X[:, 0:1, :, 0:1]))\n",
    "        u_y = torch.diagonal(torch.squeeze(U_X[:, 0:1, :, 1:2]))\n",
    "        u_z = torch.diagonal(torch.squeeze(U_X[:, 0:1, :, 2:3]))\n",
    "        u_t = torch.diagonal(torch.squeeze(U_X[:, 0:1, :, 3:4]))\n",
    "        \n",
    "        v_x = torch.diagonal(torch.squeeze(U_X[:, 1:2, :, 0:1]))\n",
    "        v_y = torch.diagonal(torch.squeeze(U_X[:, 1:2, :, 1:2]))\n",
    "        v_z = torch.diagonal(torch.squeeze(U_X[:, 1:2, :, 2:3]))\n",
    "        v_t = torch.diagonal(torch.squeeze(U_X[:, 1:2, :, 3:4]))\n",
    "        \n",
    "        w_x = torch.diagonal(torch.squeeze(U_X[:, 2:3, :, 0:1]))\n",
    "        w_y = torch.diagonal(torch.squeeze(U_X[:, 2:3, :, 1:2]))\n",
    "        w_z = torch.diagonal(torch.squeeze(U_X[:, 2:3, :, 2:3]))\n",
    "        w_t = torch.diagonal(torch.squeeze(U_X[:, 2:3, :, 3:4]))\n",
    "        \n",
    "        if second_order:\n",
    "            return u, v, w, u_x, u_y, u_z, u_t, v_x, v_y, v_z, v_t, w_x, w_y, w_z, w_t\n",
    "        \n",
    "        return u_x, v_y, w_z\n",
    "    \n",
    "    # way sub-optimal, the first order jacobian should really be computed once\n",
    "    # maybe pytorch is doing this lazy, but still, sub-optimal\n",
    "    def second_order(X):\n",
    "        U_XX = torch.autograd.functional.jacobian(first_order, X)\n",
    "        \n",
    "        u_xx = torch.diagonal(torch.squeeze(U_XX[0][:, :, 0:1]))\n",
    "        v_xx = torch.diagonal(torch.squeeze(U_XX[1][:, :, 0:1]))\n",
    "        w_xx = torch.diagonal(torch.squeeze(U_XX[2][:, :, 0:1]))\n",
    "        \n",
    "        u_yy = torch.diagonal(torch.squeeze(U_XX[0][:, :, 1:2]))\n",
    "        v_yy = torch.diagonal(torch.squeeze(U_XX[1][:, :, 1:2]))\n",
    "        w_yy = torch.diagonal(torch.squeeze(U_XX[2][:, :, 1:2]))\n",
    "        \n",
    "        u_zz = torch.diagonal(torch.squeeze(U_XX[0][:, :, 2:3]))\n",
    "        v_zz = torch.diagonal(torch.squeeze(U_XX[1][:, :, 2:3]))\n",
    "        w_zz = torch.diagonal(torch.squeeze(U_XX[2][:, :, 2:3]))\n",
    "        \n",
    "        return u_xx, u_yy, u_zz, v_xx, v_yy, v_zz, w_xx, w_yy, w_zz\n",
    "    \n",
    "    u, v, w, u_x, u_y, u_z, u_t, v_x, v_y, v_z, v_t, w_x, w_y, w_z, w_t = first_order(X, second_order=True)\n",
    "    u_xx, u_yy, u_zz, v_xx, v_yy, v_zz, w_xx, w_yy, w_zz = second_order(X)\n",
    "    \n",
    "    u_ = u_t + u * u_x + v * u_y + w * u_z - re * (u_xx + u_yy + u_zz)\n",
    "    v_ = v_t + u * v_x + v * v_y + w * v_z - re * (v_xx + v_yy + v_zz)\n",
    "    w_ = w_t + u * w_x + v * w_y + w * w_z - re * (w_xx + w_yy + w_zz)\n",
    "    \n",
    "    U = torch.stack((u_, v_, w_), axis=1)\n",
    "    \n",
    "    return U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123278b4-2725-44cd-a98f-ca1ab3cb50cd",
   "metadata": {},
   "source": [
    "#### d) The total loss function\n",
    "\n",
    "Summed-up from all previously defined residuals. Given how input $X$ was produced, it contains both samples from main domain as well as singular values used to compute both IC and BC. We need to carefully route the computation to the right residual function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c35f2593-bf05-4c94-89da-b5f261554a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Computes the loss based on all residual terms.\"\"\"\n",
    "    \n",
    "    x0 = X[:, 0:1] == 0.\n",
    "    x1 = X[:, 0:1] == 1.\n",
    "    xl_ = torch.logical_or(x0, x1)\n",
    "    xl_ = torch.cat((xl_,) * 4, axis=1)\n",
    "    xl = torch.masked_select(X, xl_).reshape(-1, 4)\n",
    "    xl_residuals = torch.mean(torch.square(Ulim_residuals(xl)))\n",
    "    \n",
    "    y0 = X[:, 1:2] == 0.\n",
    "    y1 = X[:, 1:2] == 1.\n",
    "    yl_ = torch.logical_or(y0, y1)\n",
    "    yl_ = torch.cat((yl_,) * 4, axis=1)\n",
    "    yl = torch.masked_select(X, yl_).reshape(-1, 4)\n",
    "    yl_residuals = torch.mean(torch.square(Ulim_residuals(yl)))\n",
    "    \n",
    "    z0 = X[:, 2:3] == 0.\n",
    "    z1 = X[:, 2:3] == 1.\n",
    "    zl_ = torch.logical_or(z0, z1)\n",
    "    zl_ = torch.cat((zl_,) * 4, axis=1)\n",
    "    zl = torch.masked_select(X, zl_).reshape(-1, 4)\n",
    "    zl_residuals = torch.mean(torch.square(Ulim_residuals(zl)))\n",
    "    \n",
    "    t0_ = X[:, 3:4] == 0.\n",
    "    t0_ = torch.cat((t0_,) * 4, axis=1)\n",
    "    t0 = torch.masked_select(X, t0_).reshape(-1, 4)\n",
    "    t0_residuals = torch.mean(torch.square(U0_residuals(t0)))\n",
    "    \n",
    "    or_ = torch.logical_or(t0_, torch.logical_or(zl_, torch.logical_or(xl_, yl_)))\n",
    "    X_not = torch.logical_not(or_)\n",
    "    X_ = torch.masked_select(X, X_not).reshape(-1, 4)\n",
    "    f_residuals = torch.mean(torch.square(f(X_)))\n",
    "    \n",
    "    # final loss is simply the sum of residuals\n",
    "    return torch.mean(torch.stack((\n",
    "        xl_residuals,\n",
    "        yl_residuals,\n",
    "        zl_residuals,\n",
    "        t0_residuals,\n",
    "        f_residuals\n",
    "    )))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5cd0b-5525-4d14-959a-9d45d0f9e424",
   "metadata": {},
   "source": [
    "#### e) Defining the model\n",
    "\n",
    "... as a simple straight-forward feed-forward MLP `depth=4` by `width=20` + `activation=nn.Tanh()` defined with PyTorch's sequential API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05a30102-99fa-48f2-8c0e-53f90daf4315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs: X = (x, y, z, t)\n",
    "# outputs: U = (u, v, w)\n",
    "\n",
    "pinn = nn.Sequential(\n",
    "    nn.Linear(4, 20, dtype=dtype),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(20, 20, dtype=dtype),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(20, 20, dtype=dtype),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(20, 20, dtype=dtype),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(20, 3, dtype=dtype),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794f736-fef6-4dc5-a63f-a1658300642d",
   "metadata": {},
   "source": [
    "## 4. LET'S FIT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311870b8-b1b2-4314-954b-2921948c9bbf",
   "metadata": {},
   "source": [
    "Let's start by sampling in both space and time, and create a 4D-meshgrid (main reason why all this is intractable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9506c52-fc24-4858-bbfa-5e8986c33856",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0.0, 1.0, steps=n, dtype=dtype).T\n",
    "y = torch.linspace(0.0, 1.0, steps=n, dtype=dtype).T\n",
    "z = torch.linspace(0.0, 1.0, steps=n, dtype=dtype).T\n",
    "t = torch.linspace(0.0, 1.0, steps=n, dtype=dtype).T\n",
    "X = torch.stack((x, y, z, t), axis=1)\n",
    "\n",
    "X = meshify(X)\n",
    "\n",
    "u0 = U0(X)[:, 0:1]\n",
    "v0 = U0(X)[:, 1:2]\n",
    "w0 = U0(X)[:, 2:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc5996a-e8ee-4673-aadc-3794730ff288",
   "metadata": {},
   "source": [
    "...and loop over epochs... And we're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb319110-b681-4516-9248-da690e27c7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X: torch.Tensor, \n",
    "        epochs: int,\n",
    "        lr: float = 1e-2):\n",
    "    \"\"\"Implements the training loop.\"\"\"\n",
    "\n",
    "    optimizer = torch.optim.SGD(pinn.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_ = loss(X)\n",
    "        loss_.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"epoch: {epoch}, loss: {loss_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3d71b-1070-4fa5-a4f5-724b6f2490c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.11308014392852783\n",
      "epoch: 1000, loss: 0.029296875\n",
      "epoch: 2000, loss: 0.029296875\n",
      "epoch: 3000, loss: 0.029296875\n"
     ]
    }
   ],
   "source": [
    "fit(X, epochs=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe892db3-f25e-4b55-acf4-f18da9d4505f",
   "metadata": {},
   "source": [
    "**But let's forget about printing anything useful. Simply untractable.**\n",
    "\n",
    "**For a more realistic and less irrelevant example, checkout `../burger_1d`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f7af9-fb4f-4ddb-935d-cc0533bcf950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
